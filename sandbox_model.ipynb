{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_tags = pickle.load(open('data/parsed_webis_train.pkl', 'rb'))\n",
    "val_texts, val_tags = pickle.load(open('data/parsed_webis_dev.pkl', 'rb'))\n",
    "test_texts, test_tags = pickle.load(open('data/parsed_webis_test.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut the long doc\n",
    "train_texts = [txt for txt in train_texts if len(txt) <= 512]\n",
    "train_tags = [t for t in train_tags if len(t) <= 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "#chunks: 2749\n",
      "average length: 57.4\n",
      "99% length: 147.0\n",
      "max length: 299.0\n",
      "B: 7674\n",
      "I: 136584\n",
      "O: 13597\n",
      "==============\n",
      "#chunks: 757\n",
      "average length: 71.0\n",
      "99% length: 176.8\n",
      "max length: 336.0\n",
      "B: 2676\n",
      "I: 45536\n",
      "O: 5566\n",
      "==============\n",
      "#chunks: 849\n",
      "average length: 61.0\n",
      "99% length: 190.5\n",
      "max length: 280.0\n",
      "B: 2577\n",
      "I: 44159\n",
      "O: 5081\n"
     ]
    }
   ],
   "source": [
    "#report stats here\n",
    "def report_stats(tags):\n",
    "    print('==============')\n",
    "    length = [len(t) for t in tags]\n",
    "    print(f'#chunks: {len(tags)}')\n",
    "    print(f'average length: {np.mean(length):.1f}')\n",
    "    print(f'99% length: {np.percentile(length, [99])[0]:.1f}')\n",
    "    print(f'max length: {np.percentile(length, [100])[0]:.1f}')\n",
    "    counter = collections.defaultdict(int)\n",
    "    \n",
    "    for t in tags:\n",
    "        for key in ['B', 'I', 'O']:\n",
    "            counter[key] += t.count(key)\n",
    "    for key in ['B', 'I', 'O']:\n",
    "        print(f'{key}: {counter[key]}')\n",
    "\n",
    "report_stats(train_tags)\n",
    "report_stats(val_tags)\n",
    "report_stats(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = train_tags + val_tags + test_tags\n",
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tgs, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tgs]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        \n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        \n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "        \n",
    "        \n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)\n",
    "test_labels = encode_tags(test_tags, test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WebisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "test_encodings.pop(\"offset_mapping\")\n",
    "\n",
    "train_dataset = WebisDataset(train_encodings, train_labels)\n",
    "val_dataset = WebisDataset(val_encodings, val_labels)\n",
    "test_dataset = WebisDataset(test_encodings, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=10)\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids.flatten()\n",
    "#     preds = pred.predictions.argmax(-1).flatten()\n",
    "#     z = zip(labels, preds)\n",
    "#     z = [item for item in z if item[0] != -100]\n",
    "#     labels = np.array([item[0] for item in z])\n",
    "#     preds = np.array([item[1] for item in z])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'acc_macro': acc,\n",
    "#         'f1_macro': f1,\n",
    "#         'p_macro': precision,\n",
    "#         'r_macro': recall\n",
    "#     }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = pred.predictions.argmax(-1).flatten()\n",
    "    z = zip(labels, preds)\n",
    "    z = [item for item in z if item[0] != -100]\n",
    "    labels = np.array([item[0] for item in z])\n",
    "    preds = np.array([item[1] for item in z])\n",
    "    \n",
    "    l_0 = np.array([1 if item[0]==0 else 0 for item in z])\n",
    "    p_0 = np.array([1 if item[1]==0 else 0 for item in z])\n",
    "    \n",
    "    l_1 = np.array([1 if item[0]==1 else 0 for item in z])\n",
    "    p_1 = np.array([1 if item[1]==1 else 0 for item in z])\n",
    "    \n",
    "    l_2 = np.array([1 if item[0]==2 else 0 for item in z])\n",
    "    p_2 = np.array([1 if item[1]==2 else 0 for item in z])\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    p_0, r_0, f1_0, _ = precision_recall_fscore_support(l_0, p_0, average='binary')\n",
    "    p_1, r_1, f1_1, _ = precision_recall_fscore_support(l_1, p_1, average='binary')\n",
    "    p_2, r_2, f1_2, _ = precision_recall_fscore_support(l_2, p_2, average='binary')\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'f1_macro': f1,\n",
    "        'p_macro': precision,\n",
    "        'r_macro': recall,\n",
    "        'f1_0': f1_0,\n",
    "        'p_0': p_0,\n",
    "        'r_0': r_0,\n",
    "        'f1_1': f1_1,\n",
    "        'p_1': p_1,\n",
    "        'r_1': r_1,\n",
    "        'f1_2': f1_2,\n",
    "        'p_2': p_2,\n",
    "        'r_2': r_2,\n",
    "    }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=30,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.005,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_macro'\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='310' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/330 04:45 < 00:18, 1.08 it/s, Epoch 28/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>P Macro</th>\n",
       "      <th>R Macro</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>P 0</th>\n",
       "      <th>R 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>P 1</th>\n",
       "      <th>R 1</th>\n",
       "      <th>F1 2</th>\n",
       "      <th>P 2</th>\n",
       "      <th>R 2</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.006700</td>\n",
       "      <td>0.995936</td>\n",
       "      <td>0.435996</td>\n",
       "      <td>0.256681</td>\n",
       "      <td>0.325364</td>\n",
       "      <td>0.346476</td>\n",
       "      <td>0.184680</td>\n",
       "      <td>0.109217</td>\n",
       "      <td>0.597557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585364</td>\n",
       "      <td>0.866873</td>\n",
       "      <td>0.441870</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>883.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.973752</td>\n",
       "      <td>0.499814</td>\n",
       "      <td>0.278781</td>\n",
       "      <td>0.324426</td>\n",
       "      <td>0.346629</td>\n",
       "      <td>0.181425</td>\n",
       "      <td>0.110235</td>\n",
       "      <td>0.512217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654919</td>\n",
       "      <td>0.863044</td>\n",
       "      <td>0.527670</td>\n",
       "      <td>0.907700</td>\n",
       "      <td>834.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.963900</td>\n",
       "      <td>0.937072</td>\n",
       "      <td>0.605136</td>\n",
       "      <td>0.306771</td>\n",
       "      <td>0.322139</td>\n",
       "      <td>0.342357</td>\n",
       "      <td>0.167611</td>\n",
       "      <td>0.109617</td>\n",
       "      <td>0.355911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752703</td>\n",
       "      <td>0.856798</td>\n",
       "      <td>0.671161</td>\n",
       "      <td>0.778400</td>\n",
       "      <td>972.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.920400</td>\n",
       "      <td>0.887855</td>\n",
       "      <td>0.724943</td>\n",
       "      <td>0.328764</td>\n",
       "      <td>0.322484</td>\n",
       "      <td>0.341528</td>\n",
       "      <td>0.143867</td>\n",
       "      <td>0.115074</td>\n",
       "      <td>0.191879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842426</td>\n",
       "      <td>0.852377</td>\n",
       "      <td>0.832704</td>\n",
       "      <td>1.078100</td>\n",
       "      <td>702.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>0.828652</td>\n",
       "      <td>0.808918</td>\n",
       "      <td>0.324867</td>\n",
       "      <td>0.321180</td>\n",
       "      <td>0.336054</td>\n",
       "      <td>0.078963</td>\n",
       "      <td>0.114765</td>\n",
       "      <td>0.060187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895637</td>\n",
       "      <td>0.848775</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>904.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.762055</td>\n",
       "      <td>0.840994</td>\n",
       "      <td>0.311916</td>\n",
       "      <td>0.327655</td>\n",
       "      <td>0.334541</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.135524</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913941</td>\n",
       "      <td>0.847441</td>\n",
       "      <td>0.991765</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>1003.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.690944</td>\n",
       "      <td>0.846015</td>\n",
       "      <td>0.305913</td>\n",
       "      <td>0.300457</td>\n",
       "      <td>0.333206</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916673</td>\n",
       "      <td>0.846825</td>\n",
       "      <td>0.999078</td>\n",
       "      <td>0.749100</td>\n",
       "      <td>1010.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.619133</td>\n",
       "      <td>0.846703</td>\n",
       "      <td>0.305666</td>\n",
       "      <td>0.282250</td>\n",
       "      <td>0.333319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916998</td>\n",
       "      <td>0.846750</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.760500</td>\n",
       "      <td>995.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.552715</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>0.305670</td>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>878.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.501290</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>0.305670</td>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>1022.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.466964</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>0.305670</td>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>0.846740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>1016.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.426425</td>\n",
       "      <td>0.874205</td>\n",
       "      <td>0.452034</td>\n",
       "      <td>0.615974</td>\n",
       "      <td>0.423679</td>\n",
       "      <td>0.425299</td>\n",
       "      <td>0.976759</td>\n",
       "      <td>0.271829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930803</td>\n",
       "      <td>0.871164</td>\n",
       "      <td>0.999209</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>989.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.386596</td>\n",
       "      <td>0.880565</td>\n",
       "      <td>0.478695</td>\n",
       "      <td>0.613889</td>\n",
       "      <td>0.445948</td>\n",
       "      <td>0.502060</td>\n",
       "      <td>0.964267</td>\n",
       "      <td>0.339382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.934025</td>\n",
       "      <td>0.877400</td>\n",
       "      <td>0.998463</td>\n",
       "      <td>0.750700</td>\n",
       "      <td>1008.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.358183</td>\n",
       "      <td>0.881903</td>\n",
       "      <td>0.483589</td>\n",
       "      <td>0.607250</td>\n",
       "      <td>0.451049</td>\n",
       "      <td>0.515725</td>\n",
       "      <td>0.942299</td>\n",
       "      <td>0.355013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935042</td>\n",
       "      <td>0.879453</td>\n",
       "      <td>0.998133</td>\n",
       "      <td>0.755500</td>\n",
       "      <td>1001.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.334998</td>\n",
       "      <td>0.889490</td>\n",
       "      <td>0.526450</td>\n",
       "      <td>0.852764</td>\n",
       "      <td>0.486374</td>\n",
       "      <td>0.562866</td>\n",
       "      <td>0.839089</td>\n",
       "      <td>0.423464</td>\n",
       "      <td>0.074920</td>\n",
       "      <td>0.826772</td>\n",
       "      <td>0.039238</td>\n",
       "      <td>0.941563</td>\n",
       "      <td>0.892431</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>1011.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.318324</td>\n",
       "      <td>0.911376</td>\n",
       "      <td>0.715577</td>\n",
       "      <td>0.881705</td>\n",
       "      <td>0.635481</td>\n",
       "      <td>0.580823</td>\n",
       "      <td>0.868120</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.612383</td>\n",
       "      <td>0.861695</td>\n",
       "      <td>0.474963</td>\n",
       "      <td>0.953525</td>\n",
       "      <td>0.915301</td>\n",
       "      <td>0.995081</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>1020.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.303057</td>\n",
       "      <td>0.918350</td>\n",
       "      <td>0.755580</td>\n",
       "      <td>0.870930</td>\n",
       "      <td>0.694136</td>\n",
       "      <td>0.590758</td>\n",
       "      <td>0.876325</td>\n",
       "      <td>0.445562</td>\n",
       "      <td>0.718301</td>\n",
       "      <td>0.811001</td>\n",
       "      <td>0.644619</td>\n",
       "      <td>0.957682</td>\n",
       "      <td>0.925462</td>\n",
       "      <td>0.992226</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>819.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.289979</td>\n",
       "      <td>0.920023</td>\n",
       "      <td>0.764073</td>\n",
       "      <td>0.862822</td>\n",
       "      <td>0.713194</td>\n",
       "      <td>0.597936</td>\n",
       "      <td>0.880196</td>\n",
       "      <td>0.452749</td>\n",
       "      <td>0.735451</td>\n",
       "      <td>0.778939</td>\n",
       "      <td>0.696562</td>\n",
       "      <td>0.958834</td>\n",
       "      <td>0.929331</td>\n",
       "      <td>0.990271</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>902.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.274931</td>\n",
       "      <td>0.921325</td>\n",
       "      <td>0.768307</td>\n",
       "      <td>0.866905</td>\n",
       "      <td>0.716141</td>\n",
       "      <td>0.600567</td>\n",
       "      <td>0.875430</td>\n",
       "      <td>0.457061</td>\n",
       "      <td>0.744685</td>\n",
       "      <td>0.795078</td>\n",
       "      <td>0.700299</td>\n",
       "      <td>0.959671</td>\n",
       "      <td>0.930207</td>\n",
       "      <td>0.991062</td>\n",
       "      <td>0.748100</td>\n",
       "      <td>1011.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.264130</td>\n",
       "      <td>0.922143</td>\n",
       "      <td>0.774186</td>\n",
       "      <td>0.861746</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.604489</td>\n",
       "      <td>0.856907</td>\n",
       "      <td>0.466942</td>\n",
       "      <td>0.757878</td>\n",
       "      <td>0.795725</td>\n",
       "      <td>0.723468</td>\n",
       "      <td>0.960191</td>\n",
       "      <td>0.932605</td>\n",
       "      <td>0.989459</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>878.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.259649</td>\n",
       "      <td>0.922905</td>\n",
       "      <td>0.778730</td>\n",
       "      <td>0.857467</td>\n",
       "      <td>0.740325</td>\n",
       "      <td>0.605487</td>\n",
       "      <td>0.864622</td>\n",
       "      <td>0.465864</td>\n",
       "      <td>0.770068</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>0.767190</td>\n",
       "      <td>0.960634</td>\n",
       "      <td>0.934813</td>\n",
       "      <td>0.987922</td>\n",
       "      <td>0.758300</td>\n",
       "      <td>998.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.251132</td>\n",
       "      <td>0.923649</td>\n",
       "      <td>0.788835</td>\n",
       "      <td>0.844823</td>\n",
       "      <td>0.758595</td>\n",
       "      <td>0.621658</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.501258</td>\n",
       "      <td>0.784009</td>\n",
       "      <td>0.776678</td>\n",
       "      <td>0.791480</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.939611</td>\n",
       "      <td>0.983046</td>\n",
       "      <td>0.761600</td>\n",
       "      <td>994.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.246987</td>\n",
       "      <td>0.925118</td>\n",
       "      <td>0.792441</td>\n",
       "      <td>0.853479</td>\n",
       "      <td>0.757087</td>\n",
       "      <td>0.626530</td>\n",
       "      <td>0.823099</td>\n",
       "      <td>0.505749</td>\n",
       "      <td>0.789195</td>\n",
       "      <td>0.797937</td>\n",
       "      <td>0.780643</td>\n",
       "      <td>0.961598</td>\n",
       "      <td>0.939401</td>\n",
       "      <td>0.984869</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>987.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.242697</td>\n",
       "      <td>0.924374</td>\n",
       "      <td>0.796227</td>\n",
       "      <td>0.837241</td>\n",
       "      <td>0.772935</td>\n",
       "      <td>0.636325</td>\n",
       "      <td>0.793187</td>\n",
       "      <td>0.531261</td>\n",
       "      <td>0.791293</td>\n",
       "      <td>0.774991</td>\n",
       "      <td>0.808296</td>\n",
       "      <td>0.961065</td>\n",
       "      <td>0.943545</td>\n",
       "      <td>0.979247</td>\n",
       "      <td>0.743600</td>\n",
       "      <td>1018.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.237449</td>\n",
       "      <td>0.927164</td>\n",
       "      <td>0.800288</td>\n",
       "      <td>0.858014</td>\n",
       "      <td>0.765308</td>\n",
       "      <td>0.638831</td>\n",
       "      <td>0.822348</td>\n",
       "      <td>0.522278</td>\n",
       "      <td>0.799470</td>\n",
       "      <td>0.810365</td>\n",
       "      <td>0.788864</td>\n",
       "      <td>0.962565</td>\n",
       "      <td>0.941328</td>\n",
       "      <td>0.984781</td>\n",
       "      <td>0.766300</td>\n",
       "      <td>987.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.239991</td>\n",
       "      <td>0.927387</td>\n",
       "      <td>0.801197</td>\n",
       "      <td>0.857968</td>\n",
       "      <td>0.766669</td>\n",
       "      <td>0.642184</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.526231</td>\n",
       "      <td>0.798790</td>\n",
       "      <td>0.808576</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.962618</td>\n",
       "      <td>0.941651</td>\n",
       "      <td>0.984540</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>979.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.234367</td>\n",
       "      <td>0.926531</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.846091</td>\n",
       "      <td>0.774634</td>\n",
       "      <td>0.649504</td>\n",
       "      <td>0.786810</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.798641</td>\n",
       "      <td>0.806710</td>\n",
       "      <td>0.790732</td>\n",
       "      <td>0.962136</td>\n",
       "      <td>0.944754</td>\n",
       "      <td>0.980170</td>\n",
       "      <td>0.772700</td>\n",
       "      <td>979.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.234768</td>\n",
       "      <td>0.922199</td>\n",
       "      <td>0.802303</td>\n",
       "      <td>0.820547</td>\n",
       "      <td>0.788617</td>\n",
       "      <td>0.649595</td>\n",
       "      <td>0.722296</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.797855</td>\n",
       "      <td>0.789821</td>\n",
       "      <td>0.806054</td>\n",
       "      <td>0.959461</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.969606</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>999.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.246726</td>\n",
       "      <td>0.930120</td>\n",
       "      <td>0.807242</td>\n",
       "      <td>0.877393</td>\n",
       "      <td>0.767886</td>\n",
       "      <td>0.647032</td>\n",
       "      <td>0.869842</td>\n",
       "      <td>0.515092</td>\n",
       "      <td>0.810985</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.800075</td>\n",
       "      <td>0.963710</td>\n",
       "      <td>0.940140</td>\n",
       "      <td>0.988493</td>\n",
       "      <td>0.929300</td>\n",
       "      <td>814.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.231821</td>\n",
       "      <td>0.929135</td>\n",
       "      <td>0.811381</td>\n",
       "      <td>0.858207</td>\n",
       "      <td>0.779007</td>\n",
       "      <td>0.661881</td>\n",
       "      <td>0.802406</td>\n",
       "      <td>0.563241</td>\n",
       "      <td>0.809087</td>\n",
       "      <td>0.827088</td>\n",
       "      <td>0.791854</td>\n",
       "      <td>0.963175</td>\n",
       "      <td>0.945127</td>\n",
       "      <td>0.981926</td>\n",
       "      <td>0.757900</td>\n",
       "      <td>998.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.227868</td>\n",
       "      <td>0.924393</td>\n",
       "      <td>0.808616</td>\n",
       "      <td>0.824626</td>\n",
       "      <td>0.797919</td>\n",
       "      <td>0.661771</td>\n",
       "      <td>0.739246</td>\n",
       "      <td>0.598994</td>\n",
       "      <td>0.803714</td>\n",
       "      <td>0.783736</td>\n",
       "      <td>0.824738</td>\n",
       "      <td>0.960364</td>\n",
       "      <td>0.950896</td>\n",
       "      <td>0.970024</td>\n",
       "      <td>0.761600</td>\n",
       "      <td>993.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.227891</td>\n",
       "      <td>0.930492</td>\n",
       "      <td>0.813794</td>\n",
       "      <td>0.868486</td>\n",
       "      <td>0.777358</td>\n",
       "      <td>0.663445</td>\n",
       "      <td>0.824613</td>\n",
       "      <td>0.554977</td>\n",
       "      <td>0.814203</td>\n",
       "      <td>0.837017</td>\n",
       "      <td>0.792601</td>\n",
       "      <td>0.963734</td>\n",
       "      <td>0.943829</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>989.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.232052</td>\n",
       "      <td>0.929321</td>\n",
       "      <td>0.813480</td>\n",
       "      <td>0.850180</td>\n",
       "      <td>0.791516</td>\n",
       "      <td>0.666526</td>\n",
       "      <td>0.809499</td>\n",
       "      <td>0.566475</td>\n",
       "      <td>0.810752</td>\n",
       "      <td>0.793770</td>\n",
       "      <td>0.828475</td>\n",
       "      <td>0.963164</td>\n",
       "      <td>0.947271</td>\n",
       "      <td>0.979599</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>860.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.233806</td>\n",
       "      <td>0.928930</td>\n",
       "      <td>0.815146</td>\n",
       "      <td>0.855030</td>\n",
       "      <td>0.786588</td>\n",
       "      <td>0.663621</td>\n",
       "      <td>0.786030</td>\n",
       "      <td>0.574201</td>\n",
       "      <td>0.819062</td>\n",
       "      <td>0.832497</td>\n",
       "      <td>0.806054</td>\n",
       "      <td>0.962755</td>\n",
       "      <td>0.946563</td>\n",
       "      <td>0.979511</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>855.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.245958</td>\n",
       "      <td>0.929971</td>\n",
       "      <td>0.815973</td>\n",
       "      <td>0.852904</td>\n",
       "      <td>0.793406</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.568451</td>\n",
       "      <td>0.816731</td>\n",
       "      <td>0.802162</td>\n",
       "      <td>0.831839</td>\n",
       "      <td>0.963467</td>\n",
       "      <td>0.947549</td>\n",
       "      <td>0.979928</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>981.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.930901</td>\n",
       "      <td>0.817135</td>\n",
       "      <td>0.865624</td>\n",
       "      <td>0.784461</td>\n",
       "      <td>0.667942</td>\n",
       "      <td>0.817425</td>\n",
       "      <td>0.564678</td>\n",
       "      <td>0.819616</td>\n",
       "      <td>0.834043</td>\n",
       "      <td>0.805680</td>\n",
       "      <td>0.963847</td>\n",
       "      <td>0.945404</td>\n",
       "      <td>0.983024</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>838.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.249939</td>\n",
       "      <td>0.931608</td>\n",
       "      <td>0.819519</td>\n",
       "      <td>0.862992</td>\n",
       "      <td>0.794106</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.831377</td>\n",
       "      <td>0.559828</td>\n",
       "      <td>0.825257</td>\n",
       "      <td>0.810967</td>\n",
       "      <td>0.840060</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>0.946633</td>\n",
       "      <td>0.982431</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>971.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.251620</td>\n",
       "      <td>0.924226</td>\n",
       "      <td>0.813150</td>\n",
       "      <td>0.833076</td>\n",
       "      <td>0.795715</td>\n",
       "      <td>0.662118</td>\n",
       "      <td>0.711777</td>\n",
       "      <td>0.618936</td>\n",
       "      <td>0.817348</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.799327</td>\n",
       "      <td>0.959985</td>\n",
       "      <td>0.951251</td>\n",
       "      <td>0.968882</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>936.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.262640</td>\n",
       "      <td>0.922701</td>\n",
       "      <td>0.810365</td>\n",
       "      <td>0.817889</td>\n",
       "      <td>0.805113</td>\n",
       "      <td>0.662644</td>\n",
       "      <td>0.709684</td>\n",
       "      <td>0.621452</td>\n",
       "      <td>0.809341</td>\n",
       "      <td>0.790731</td>\n",
       "      <td>0.828849</td>\n",
       "      <td>0.959110</td>\n",
       "      <td>0.953253</td>\n",
       "      <td>0.965039</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>993.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.260227</td>\n",
       "      <td>0.926364</td>\n",
       "      <td>0.815732</td>\n",
       "      <td>0.839380</td>\n",
       "      <td>0.796528</td>\n",
       "      <td>0.666995</td>\n",
       "      <td>0.739235</td>\n",
       "      <td>0.607618</td>\n",
       "      <td>0.819127</td>\n",
       "      <td>0.828681</td>\n",
       "      <td>0.809791</td>\n",
       "      <td>0.961074</td>\n",
       "      <td>0.950223</td>\n",
       "      <td>0.972176</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>857.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.271512</td>\n",
       "      <td>0.929469</td>\n",
       "      <td>0.818003</td>\n",
       "      <td>0.850240</td>\n",
       "      <td>0.796383</td>\n",
       "      <td>0.669975</td>\n",
       "      <td>0.789089</td>\n",
       "      <td>0.582106</td>\n",
       "      <td>0.821092</td>\n",
       "      <td>0.813118</td>\n",
       "      <td>0.829223</td>\n",
       "      <td>0.962943</td>\n",
       "      <td>0.948512</td>\n",
       "      <td>0.977820</td>\n",
       "      <td>0.841200</td>\n",
       "      <td>899.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.288348</td>\n",
       "      <td>0.931589</td>\n",
       "      <td>0.820570</td>\n",
       "      <td>0.864277</td>\n",
       "      <td>0.791294</td>\n",
       "      <td>0.673844</td>\n",
       "      <td>0.816786</td>\n",
       "      <td>0.573482</td>\n",
       "      <td>0.823773</td>\n",
       "      <td>0.829231</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>0.964092</td>\n",
       "      <td>0.946812</td>\n",
       "      <td>0.982014</td>\n",
       "      <td>0.775900</td>\n",
       "      <td>975.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.287458</td>\n",
       "      <td>0.931905</td>\n",
       "      <td>0.822319</td>\n",
       "      <td>0.862447</td>\n",
       "      <td>0.794920</td>\n",
       "      <td>0.678310</td>\n",
       "      <td>0.811562</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.827807</td>\n",
       "      <td>0.821001</td>\n",
       "      <td>0.964258</td>\n",
       "      <td>0.947971</td>\n",
       "      <td>0.981114</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>919.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.290062</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.819951</td>\n",
       "      <td>0.849954</td>\n",
       "      <td>0.799827</td>\n",
       "      <td>0.674895</td>\n",
       "      <td>0.789271</td>\n",
       "      <td>0.589472</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>0.811067</td>\n",
       "      <td>0.832586</td>\n",
       "      <td>0.963272</td>\n",
       "      <td>0.949524</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.743600</td>\n",
       "      <td>1018.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.297572</td>\n",
       "      <td>0.926550</td>\n",
       "      <td>0.815671</td>\n",
       "      <td>0.841037</td>\n",
       "      <td>0.795364</td>\n",
       "      <td>0.665476</td>\n",
       "      <td>0.743022</td>\n",
       "      <td>0.602587</td>\n",
       "      <td>0.820348</td>\n",
       "      <td>0.830398</td>\n",
       "      <td>0.810538</td>\n",
       "      <td>0.961188</td>\n",
       "      <td>0.949691</td>\n",
       "      <td>0.972966</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>1020.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.304306</td>\n",
       "      <td>0.929785</td>\n",
       "      <td>0.818801</td>\n",
       "      <td>0.847148</td>\n",
       "      <td>0.800643</td>\n",
       "      <td>0.675932</td>\n",
       "      <td>0.791466</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>0.817334</td>\n",
       "      <td>0.800215</td>\n",
       "      <td>0.835202</td>\n",
       "      <td>0.963139</td>\n",
       "      <td>0.949762</td>\n",
       "      <td>0.976897</td>\n",
       "      <td>0.921100</td>\n",
       "      <td>821.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.323437</td>\n",
       "      <td>0.932519</td>\n",
       "      <td>0.817448</td>\n",
       "      <td>0.876837</td>\n",
       "      <td>0.781369</td>\n",
       "      <td>0.669518</td>\n",
       "      <td>0.859476</td>\n",
       "      <td>0.548329</td>\n",
       "      <td>0.818062</td>\n",
       "      <td>0.827283</td>\n",
       "      <td>0.809043</td>\n",
       "      <td>0.964765</td>\n",
       "      <td>0.943751</td>\n",
       "      <td>0.986736</td>\n",
       "      <td>0.772100</td>\n",
       "      <td>980.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.320592</td>\n",
       "      <td>0.932928</td>\n",
       "      <td>0.819167</td>\n",
       "      <td>0.875478</td>\n",
       "      <td>0.785521</td>\n",
       "      <td>0.670023</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>0.549946</td>\n",
       "      <td>0.822405</td>\n",
       "      <td>0.824568</td>\n",
       "      <td>0.820254</td>\n",
       "      <td>0.965073</td>\n",
       "      <td>0.944684</td>\n",
       "      <td>0.986362</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>990.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.299791</td>\n",
       "      <td>0.922161</td>\n",
       "      <td>0.811383</td>\n",
       "      <td>0.819223</td>\n",
       "      <td>0.804380</td>\n",
       "      <td>0.664293</td>\n",
       "      <td>0.694145</td>\n",
       "      <td>0.636903</td>\n",
       "      <td>0.811113</td>\n",
       "      <td>0.809453</td>\n",
       "      <td>0.812780</td>\n",
       "      <td>0.958741</td>\n",
       "      <td>0.954071</td>\n",
       "      <td>0.963457</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>993.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.303077</td>\n",
       "      <td>0.922701</td>\n",
       "      <td>0.812341</td>\n",
       "      <td>0.823421</td>\n",
       "      <td>0.802902</td>\n",
       "      <td>0.660253</td>\n",
       "      <td>0.702615</td>\n",
       "      <td>0.622709</td>\n",
       "      <td>0.817877</td>\n",
       "      <td>0.815145</td>\n",
       "      <td>0.820628</td>\n",
       "      <td>0.958893</td>\n",
       "      <td>0.952504</td>\n",
       "      <td>0.965368</td>\n",
       "      <td>0.764500</td>\n",
       "      <td>990.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.320533</td>\n",
       "      <td>0.930901</td>\n",
       "      <td>0.817633</td>\n",
       "      <td>0.860565</td>\n",
       "      <td>0.790315</td>\n",
       "      <td>0.670066</td>\n",
       "      <td>0.819647</td>\n",
       "      <td>0.566655</td>\n",
       "      <td>0.818977</td>\n",
       "      <td>0.815487</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.963856</td>\n",
       "      <td>0.946561</td>\n",
       "      <td>0.981795</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1009.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.329851</td>\n",
       "      <td>0.932984</td>\n",
       "      <td>0.823365</td>\n",
       "      <td>0.869121</td>\n",
       "      <td>0.791876</td>\n",
       "      <td>0.681411</td>\n",
       "      <td>0.823365</td>\n",
       "      <td>0.581207</td>\n",
       "      <td>0.823753</td>\n",
       "      <td>0.836609</td>\n",
       "      <td>0.811286</td>\n",
       "      <td>0.964932</td>\n",
       "      <td>0.947391</td>\n",
       "      <td>0.983134</td>\n",
       "      <td>0.742100</td>\n",
       "      <td>1020.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.342892</td>\n",
       "      <td>0.932296</td>\n",
       "      <td>0.822467</td>\n",
       "      <td>0.861544</td>\n",
       "      <td>0.797099</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.817836</td>\n",
       "      <td>0.579950</td>\n",
       "      <td>0.824119</td>\n",
       "      <td>0.818349</td>\n",
       "      <td>0.829970</td>\n",
       "      <td>0.964631</td>\n",
       "      <td>0.948447</td>\n",
       "      <td>0.981377</td>\n",
       "      <td>0.953100</td>\n",
       "      <td>794.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.346733</td>\n",
       "      <td>0.930287</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.849237</td>\n",
       "      <td>0.799014</td>\n",
       "      <td>0.680065</td>\n",
       "      <td>0.786827</td>\n",
       "      <td>0.598814</td>\n",
       "      <td>0.815819</td>\n",
       "      <td>0.810701</td>\n",
       "      <td>0.821001</td>\n",
       "      <td>0.963516</td>\n",
       "      <td>0.950184</td>\n",
       "      <td>0.977227</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>997.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.336017</td>\n",
       "      <td>0.927945</td>\n",
       "      <td>0.815391</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.795217</td>\n",
       "      <td>0.670511</td>\n",
       "      <td>0.767307</td>\n",
       "      <td>0.595401</td>\n",
       "      <td>0.813502</td>\n",
       "      <td>0.811988</td>\n",
       "      <td>0.815022</td>\n",
       "      <td>0.962160</td>\n",
       "      <td>0.949437</td>\n",
       "      <td>0.975228</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>958.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.340676</td>\n",
       "      <td>0.927777</td>\n",
       "      <td>0.817147</td>\n",
       "      <td>0.836433</td>\n",
       "      <td>0.803501</td>\n",
       "      <td>0.676626</td>\n",
       "      <td>0.759106</td>\n",
       "      <td>0.610313</td>\n",
       "      <td>0.812844</td>\n",
       "      <td>0.798486</td>\n",
       "      <td>0.827728</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.951708</td>\n",
       "      <td>0.972461</td>\n",
       "      <td>0.771100</td>\n",
       "      <td>981.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.347113</td>\n",
       "      <td>0.924244</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>0.824038</td>\n",
       "      <td>0.806441</td>\n",
       "      <td>0.671110</td>\n",
       "      <td>0.713938</td>\n",
       "      <td>0.633130</td>\n",
       "      <td>0.812211</td>\n",
       "      <td>0.804324</td>\n",
       "      <td>0.820254</td>\n",
       "      <td>0.959858</td>\n",
       "      <td>0.953852</td>\n",
       "      <td>0.965939</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>993.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.351958</td>\n",
       "      <td>0.931794</td>\n",
       "      <td>0.823334</td>\n",
       "      <td>0.854419</td>\n",
       "      <td>0.802057</td>\n",
       "      <td>0.685227</td>\n",
       "      <td>0.800144</td>\n",
       "      <td>0.599174</td>\n",
       "      <td>0.820503</td>\n",
       "      <td>0.812683</td>\n",
       "      <td>0.828475</td>\n",
       "      <td>0.964271</td>\n",
       "      <td>0.950429</td>\n",
       "      <td>0.978522</td>\n",
       "      <td>0.759100</td>\n",
       "      <td>997.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.371126</td>\n",
       "      <td>0.933040</td>\n",
       "      <td>0.820344</td>\n",
       "      <td>0.868710</td>\n",
       "      <td>0.791335</td>\n",
       "      <td>0.678464</td>\n",
       "      <td>0.849271</td>\n",
       "      <td>0.564858</td>\n",
       "      <td>0.817407</td>\n",
       "      <td>0.810206</td>\n",
       "      <td>0.824738</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>0.984408</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>820.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>0.783758</td>\n",
       "      <td>0.677667</td>\n",
       "      <td>0.816409</td>\n",
       "      <td>0.579231</td>\n",
       "      <td>0.817513</td>\n",
       "      <td>0.848753</td>\n",
       "      <td>0.788490</td>\n",
       "      <td>0.964416</td>\n",
       "      <td>0.946011</td>\n",
       "      <td>0.983551</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>981.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.933913</td>\n",
       "      <td>0.822175</td>\n",
       "      <td>0.874978</td>\n",
       "      <td>0.789750</td>\n",
       "      <td>0.680966</td>\n",
       "      <td>0.857572</td>\n",
       "      <td>0.564678</td>\n",
       "      <td>0.819985</td>\n",
       "      <td>0.821214</td>\n",
       "      <td>0.818759</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.946148</td>\n",
       "      <td>0.985813</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>1019.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.382412</td>\n",
       "      <td>0.931849</td>\n",
       "      <td>0.820742</td>\n",
       "      <td>0.864543</td>\n",
       "      <td>0.791259</td>\n",
       "      <td>0.676018</td>\n",
       "      <td>0.818437</td>\n",
       "      <td>0.575817</td>\n",
       "      <td>0.821913</td>\n",
       "      <td>0.828149</td>\n",
       "      <td>0.815770</td>\n",
       "      <td>0.964296</td>\n",
       "      <td>0.947042</td>\n",
       "      <td>0.982190</td>\n",
       "      <td>0.757900</td>\n",
       "      <td>998.811000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=310, training_loss=0.2345658930559312, metrics={'train_runtime': 306.2719, 'train_samples_per_second': 1.077, 'total_flos': 10183685664087936, 'epoch': 28.18})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 28.18,\n",
       " 'eval_acc': 0.9329837480010413,\n",
       " 'eval_f1_0': 0.6814112690889943,\n",
       " 'eval_f1_1': 0.8237526086131665,\n",
       " 'eval_f1_2': 0.964931565901498,\n",
       " 'eval_f1_macro': 0.8233651478678863,\n",
       " 'eval_loss': 0.32985052466392517,\n",
       " 'eval_p_0': 0.8233647238483075,\n",
       " 'eval_p_1': 0.8366088631984586,\n",
       " 'eval_p_2': 0.9473906970838447,\n",
       " 'eval_p_macro': 0.869121428043537,\n",
       " 'eval_r_0': 0.5812073302191879,\n",
       " 'eval_r_1': 0.8112855007473841,\n",
       " 'eval_r_2': 0.983134223471539,\n",
       " 'eval_r_macro': 0.7918756848127037,\n",
       " 'eval_runtime': 0.7806,\n",
       " 'eval_samples_per_second': 969.75}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 28.18,\n",
       " 'eval_acc': 0.9418337611208677,\n",
       " 'eval_f1_0': 0.7113955880672077,\n",
       " 'eval_f1_1': 0.853443201883461,\n",
       " 'eval_f1_2': 0.9693054751191696,\n",
       " 'eval_f1_macro': 0.8447147550232795,\n",
       " 'eval_loss': 0.284835547208786,\n",
       " 'eval_p_0': 0.8484187568157033,\n",
       " 'eval_p_1': 0.8630952380952381,\n",
       " 'eval_p_2': 0.9536917311359004,\n",
       " 'eval_p_macro': 0.8884019086822806,\n",
       " 'eval_r_0': 0.6124778586892344,\n",
       " 'eval_r_1': 0.8440046565774156,\n",
       " 'eval_r_2': 0.9854389818610023,\n",
       " 'eval_r_macro': 0.8139738323758842,\n",
       " 'eval_runtime': 0.7375,\n",
       " 'eval_samples_per_second': 1151.21}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
